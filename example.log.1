DEBUG main org.spark-project.jetty.util.component.AbstractLifeCycle - starting org.spark-project.jetty.server.Server@de579ff
INFO main org.spark-project.jetty.server.Server - jetty-8.y.z-SNAPSHOT
DEBUG main org.spark-project.jetty.util.component.AbstractLifeCycle - starting org.spark-project.jetty.server.handler.HandlerList@6b27b2d0
DEBUG main org.spark-project.jetty.util.component.AbstractLifeCycle - starting org.spark-project.jetty.server.handler.ResourceHandler@4fb64e14
DEBUG main org.spark-project.jetty.server.handler.AbstractHandler - starting org.spark-project.jetty.server.handler.ResourceHandler@4fb64e14
DEBUG main org.spark-project.jetty.util.component.AbstractLifeCycle - STARTED org.spark-project.jetty.server.handler.ResourceHandler@4fb64e14
DEBUG main org.spark-project.jetty.util.component.AbstractLifeCycle - starting org.spark-project.jetty.server.handler.DefaultHandler@118041c7
DEBUG main org.spark-project.jetty.server.handler.AbstractHandler - starting org.spark-project.jetty.server.handler.DefaultHandler@118041c7
DEBUG main org.spark-project.jetty.util.component.AbstractLifeCycle - STARTED org.spark-project.jetty.server.handler.DefaultHandler@118041c7
DEBUG main org.spark-project.jetty.server.handler.AbstractHandler - starting org.spark-project.jetty.server.handler.HandlerList@6b27b2d0
DEBUG main org.spark-project.jetty.util.component.AbstractLifeCycle - STARTED org.spark-project.jetty.server.handler.HandlerList@6b27b2d0
DEBUG main org.spark-project.jetty.server.handler.AbstractHandler - starting org.spark-project.jetty.server.Server@de579ff
DEBUG main org.spark-project.jetty.util.component.AbstractLifeCycle - starting qtp2037498537{8<=0<=0/254,-1}
DEBUG main org.spark-project.jetty.util.component.AbstractLifeCycle - STARTED qtp2037498537{8<=7<=8/254,0}
DEBUG main org.spark-project.jetty.util.component.AbstractLifeCycle - starting SocketConnector@0.0.0.0:0
DEBUG main org.spark-project.jetty.util.component.AbstractLifeCycle - starting null/null
DEBUG main org.spark-project.jetty.util.component.AbstractLifeCycle - STARTED PooledBuffers [0/1024@6144,0/1024@16384,0/1024@-]/PooledBuffers [0/1024@6144,0/1024@32768,0/1024@-]
INFO main org.spark-project.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:44558
DEBUG main org.spark-project.jetty.util.component.AbstractLifeCycle - STARTED SocketConnector@0.0.0.0:44558
DEBUG main org.spark-project.jetty.util.component.AbstractLifeCycle - STARTED org.spark-project.jetty.server.Server@de579ff
INFO main org.apache.spark.util.Utils - Successfully started service 'HTTP file server' on port 44558.
DEBUG main org.apache.spark.HttpFileServer - HTTP file server started at: http://172.16.64.55:44558
INFO main org.apache.spark.SparkContext - Added JAR /home/jonas.kemper/rheem/rheem-distro/target/rheem-distro_2.11-0.3.1-SNAPSHOT-distro/rheem-distro_2.11-0.3.1-SNAPSHOT/rheem-spark_2.11-0.3.1-SNAPSHOT.jar at http://172.16.64.55:44558/jars/rheem-spark_2.11-0.3.1-SNAPSHOT.jar with timestamp 1500127483963
INFO main org.apache.spark.SparkContext - Added JAR /home/jonas.kemper/rheem/rheem-distro/target/rheem-distro_2.11-0.3.1-SNAPSHOT-distro/rheem-distro_2.11-0.3.1-SNAPSHOT/rheem-basic-0.3.1-SNAPSHOT.jar at http://172.16.64.55:44558/jars/rheem-basic-0.3.1-SNAPSHOT.jar with timestamp 1500127483965
INFO main org.apache.spark.SparkContext - Added JAR /home/jonas.kemper/rheem/rheem-distro/target/rheem-distro_2.11-0.3.1-SNAPSHOT-distro/rheem-distro_2.11-0.3.1-SNAPSHOT/rheem-core-0.3.1-SNAPSHOT.jar at http://172.16.64.55:44558/jars/rheem-core-0.3.1-SNAPSHOT.jar with timestamp 1500127483968
INFO main org.apache.spark.SparkContext - Added JAR /home/jonas.kemper/rheem-benchmark/target/rheem-benchmark-1.0-SNAPSHOT.jar at http://172.16.64.55:44558/jars/rheem-benchmark-1.0-SNAPSHOT.jar with timestamp 1500127484028
INFO main org.apache.spark.SparkContext - Added JAR /home/jonas.kemper/rheem/rheem-distro/target/rheem-distro_2.11-0.3.1-SNAPSHOT-distro/rheem-distro_2.11-0.3.1-SNAPSHOT/rheem-api_2.11-0.3.1-SNAPSHOT.jar at http://172.16.64.55:44558/jars/rheem-api_2.11-0.3.1-SNAPSHOT.jar with timestamp 1500127484031
INFO main org.qcri.rheem.core.platform.CrossPlatformExecutor - Having SparkExecutor[0] execute ExecutionStage[T[SparkTextFileSource[Load hdfs://thor01/data/rdf/dbpedia/dbpedia-2016-04/page_links_en_uris_de.ttl]]]:
>     SparkTextFileSource[Load hdfs://thor01/data/rdf/dbpedia/dbpedia-2016-04/page_links_en_uris_de.ttl] => RddChannel => SparkCollect[convert output@SparkTextFileSource[Load hdfs://thor01/data/rdf/dbpedia/dbpedia-2016-04/page_links_en_uris_de.ttl]]
> Out SparkCollect[convert output@SparkTextFileSource[Load hdfs://thor01/data/rdf/dbpedia/dbpedia-2016-04/page_links_en_uris_de.ttl]] => CollectionChannel
INFO dispatcher-event-loop-9 org.apache.spark.deploy.client.AppClient$ClientEndpoint - Executor updated: app-20170715160442-0028/3 is now RUNNING
INFO dispatcher-event-loop-2 org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend - Registered executor NettyRpcEndpointRef(null) (thor04.hpi.uni-potsdam.de:33750) with ID 0
INFO dispatcher-event-loop-16 org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager thor04.hpi.uni-potsdam.de:42815 with 14.2 GB RAM, BlockManagerId(0, thor04.hpi.uni-potsdam.de, 42815)
INFO dispatcher-event-loop-2 org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend - Registered executor NettyRpcEndpointRef(null) (thor03.hpi.uni-potsdam.de:42492) with ID 2
INFO dispatcher-event-loop-2 org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend - Registered executor NettyRpcEndpointRef(null) (thor02.hpi.uni-potsdam.de:41478) with ID 1
INFO dispatcher-event-loop-13 org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager thor03.hpi.uni-potsdam.de:35741 with 14.2 GB RAM, BlockManagerId(2, thor03.hpi.uni-potsdam.de, 35741)
INFO dispatcher-event-loop-15 org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager thor02.hpi.uni-potsdam.de:40139 with 14.2 GB RAM, BlockManagerId(1, thor02.hpi.uni-potsdam.de, 40139)
DEBUG IPC Client (872306601) connection to thor01/172.16.64.55:8020 from jonas.kemper org.apache.hadoop.ipc.Client - IPC Client (872306601) connection to thor01/172.16.64.55:8020 from jonas.kemper: closed
DEBUG IPC Client (872306601) connection to thor01/172.16.64.55:8020 from jonas.kemper org.apache.hadoop.ipc.Client - IPC Client (872306601) connection to thor01/172.16.64.55:8020 from jonas.kemper: stopped, remaining connections 0
INFO main org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 208.4 KB, free 5.1 GB)
DEBUG main org.apache.spark.storage.BlockManager - Put block broadcast_0 locally took  1446 ms
DEBUG main org.apache.spark.storage.BlockManager - Putting block broadcast_0 without replication took  1446 ms
INFO main org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 19.3 KB, free 5.1 GB)
INFO dispatcher-event-loop-4 org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 172.16.64.55:41258 (size: 19.3 KB, free: 5.1 GB)
DEBUG main org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
DEBUG main org.apache.spark.storage.BlockManager - Told master about block broadcast_0_piece0
DEBUG main org.apache.spark.storage.BlockManager - Put block broadcast_0_piece0 locally took  3 ms
DEBUG main org.apache.spark.storage.BlockManager - Putting block broadcast_0_piece0 without replication took  3 ms
INFO main org.apache.spark.SparkContext - Created broadcast 0 from textFile at SparkTextFileSource.java:52
DEBUG main org.apache.spark.util.ClosureCleaner - +++ Cleaning closure <function1> (org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$33) +++
DEBUG main org.apache.spark.util.ClosureCleaner -  + declared fields: 2
DEBUG main org.apache.spark.util.ClosureCleaner -      public static final long org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$33.serialVersionUID
DEBUG main org.apache.spark.util.ClosureCleaner -      private final org.apache.spark.SparkContext$$anonfun$hadoopFile$1 org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$33.$outer
DEBUG main org.apache.spark.util.ClosureCleaner -  + declared methods: 2
DEBUG main org.apache.spark.util.ClosureCleaner -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$33.apply(java.lang.Object)
DEBUG main org.apache.spark.util.ClosureCleaner -      public final void org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$33.apply(org.apache.hadoop.mapred.JobConf)
DEBUG main org.apache.spark.util.ClosureCleaner -  + inner classes: 0
DEBUG main org.apache.spark.util.ClosureCleaner -  + outer classes: 2
DEBUG main org.apache.spark.util.ClosureCleaner -      org.apache.spark.SparkContext$$anonfun$hadoopFile$1
DEBUG main org.apache.spark.util.ClosureCleaner -      org.apache.spark.SparkContext
DEBUG main org.apache.spark.util.ClosureCleaner -  + outer objects: 2
DEBUG main org.apache.spark.util.ClosureCleaner -      <function0>
DEBUG main org.apache.spark.util.ClosureCleaner -      org.apache.spark.SparkContext@376b5cb2
DEBUG main org.apache.spark.util.ClosureCleaner -  + populating accessed fields because this is the starting closure
DEBUG main org.apache.spark.util.ClosureCleaner -  + fields accessed by starting closure: 2
DEBUG main org.apache.spark.util.ClosureCleaner -      (class org.apache.spark.SparkContext,Set())
DEBUG main org.apache.spark.util.ClosureCleaner -      (class org.apache.spark.SparkContext$$anonfun$hadoopFile$1,Set(path$6))
DEBUG main org.apache.spark.util.ClosureCleaner -  + outermost object is not a closure, so do not clone it: (class org.apache.spark.SparkContext,org.apache.spark.SparkContext@376b5cb2)
DEBUG main org.apache.spark.util.ClosureCleaner -  + cloning the object <function0> of class org.apache.spark.SparkContext$$anonfun$hadoopFile$1
DEBUG main org.apache.spark.util.ClosureCleaner -  + cleaning cloned closure <function0> recursively (org.apache.spark.SparkContext$$anonfun$hadoopFile$1)
DEBUG main org.apache.spark.util.ClosureCleaner - +++ Cleaning closure <function0> (org.apache.spark.SparkContext$$anonfun$hadoopFile$1) +++
DEBUG main org.apache.spark.util.ClosureCleaner -  + declared fields: 7
DEBUG main org.apache.spark.util.ClosureCleaner -      public static final long org.apache.spark.SparkContext$$anonfun$hadoopFile$1.serialVersionUID
DEBUG main org.apache.spark.util.ClosureCleaner -      private final org.apache.spark.SparkContext org.apache.spark.SparkContext$$anonfun$hadoopFile$1.$outer
DEBUG main org.apache.spark.util.ClosureCleaner -      public final java.lang.String org.apache.spark.SparkContext$$anonfun$hadoopFile$1.path$6
DEBUG main org.apache.spark.util.ClosureCleaner -      private final java.lang.Class org.apache.spark.SparkContext$$anonfun$hadoopFile$1.inputFormatClass$1
DEBUG main org.apache.spark.util.ClosureCleaner -      private final java.lang.Class org.apache.spark.SparkContext$$anonfun$hadoopFile$1.keyClass$1
DEBUG main org.apache.spark.util.ClosureCleaner -      private final java.lang.Class org.apache.spark.SparkContext$$anonfun$hadoopFile$1.valueClass$1
DEBUG main org.apache.spark.util.ClosureCleaner -      private final int org.apache.spark.SparkContext$$anonfun$hadoopFile$1.minPartitions$3
DEBUG main org.apache.spark.util.ClosureCleaner -  + declared methods: 2
DEBUG main org.apache.spark.util.ClosureCleaner -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply()
DEBUG main org.apache.spark.util.ClosureCleaner -      public final org.apache.spark.rdd.HadoopRDD org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply()
DEBUG main org.apache.spark.util.ClosureCleaner -  + inner classes: 1
DEBUG main org.apache.spark.util.ClosureCleaner -      org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$33
DEBUG main org.apache.spark.util.ClosureCleaner -  + outer classes: 1
DEBUG main org.apache.spark.util.ClosureCleaner -      org.apache.spark.SparkContext
DEBUG main org.apache.spark.util.ClosureCleaner -  + outer objects: 1
DEBUG main org.apache.spark.util.ClosureCleaner -      org.apache.spark.SparkContext@376b5cb2
DEBUG main org.apache.spark.util.ClosureCleaner -  + fields accessed by starting closure: 2
DEBUG main org.apache.spark.util.ClosureCleaner -      (class org.apache.spark.SparkContext,Set())
DEBUG main org.apache.spark.util.ClosureCleaner -      (class org.apache.spark.SparkContext$$anonfun$hadoopFile$1,Set(path$6))
DEBUG main org.apache.spark.util.ClosureCleaner -  + outermost object is not a closure, so do not clone it: (class org.apache.spark.SparkContext,org.apache.spark.SparkContext@376b5cb2)
DEBUG main org.apache.spark.util.ClosureCleaner -  + the starting closure doesn't actually need org.apache.spark.SparkContext@376b5cb2, so we null it out
DEBUG main org.apache.spark.util.ClosureCleaner -  +++ closure <function0> (org.apache.spark.SparkContext$$anonfun$hadoopFile$1) is now cleaned +++
DEBUG main org.apache.spark.util.ClosureCleaner -  +++ closure <function1> (org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$33) is now cleaned +++
DEBUG main org.apache.spark.util.ClosureCleaner - +++ Cleaning closure <function1> (org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$9) +++
DEBUG main org.apache.spark.util.ClosureCleaner -  + declared fields: 1
DEBUG main org.apache.spark.util.ClosureCleaner -      public static final long org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$9.serialVersionUID
DEBUG main org.apache.spark.util.ClosureCleaner -  + declared methods: 2
DEBUG main org.apache.spark.util.ClosureCleaner -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$9.apply(java.lang.Object)
DEBUG main org.apache.spark.util.ClosureCleaner -      public final java.lang.String org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$9.apply(scala.Tuple2)
DEBUG main org.apache.spark.util.ClosureCleaner -  + inner classes: 0
DEBUG main org.apache.spark.util.ClosureCleaner -  + outer classes: 0
DEBUG main org.apache.spark.util.ClosureCleaner -  + outer objects: 0
DEBUG main org.apache.spark.util.ClosureCleaner -  + populating accessed fields because this is the starting closure
DEBUG main org.apache.spark.util.ClosureCleaner -  + fields accessed by starting closure: 0
DEBUG main org.apache.spark.util.ClosureCleaner -  + there are no enclosing objects!
DEBUG main org.apache.spark.util.ClosureCleaner -  +++ closure <function1> (org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$9) is now cleaned +++
DEBUG main org.apache.spark.util.ClosureCleaner - +++ Cleaning closure <function1> (org.apache.spark.api.java.JavaRDD$$anonfun$filter$1) +++
DEBUG main org.apache.spark.util.ClosureCleaner -  + declared fields: 2
DEBUG main org.apache.spark.util.ClosureCleaner -      public static final long org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.serialVersionUID
DEBUG main org.apache.spark.util.ClosureCleaner -      private final org.apache.spark.api.java.function.Function org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.f$1
DEBUG main org.apache.spark.util.ClosureCleaner -  + declared methods: 2
DEBUG main org.apache.spark.util.ClosureCleaner -      public final java.lang.Object org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(java.lang.Object)
DEBUG main org.apache.spark.util.ClosureCleaner -      public final boolean org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(java.lang.Object)
DEBUG main org.apache.spark.util.ClosureCleaner -  + inner classes: 0
DEBUG main org.apache.spark.util.ClosureCleaner -  + outer classes: 0
DEBUG main org.apache.spark.util.ClosureCleaner -  + outer objects: 0
DEBUG main org.apache.spark.util.ClosureCleaner -  + populating accessed fields because this is the starting closure
DEBUG main org.apache.spark.util.ClosureCleaner -  + fields accessed by starting closure: 0
DEBUG main org.apache.spark.util.ClosureCleaner -  + there are no enclosing objects!
DEBUG main org.apache.spark.util.ClosureCleaner -  +++ closure <function1> (org.apache.spark.api.java.JavaRDD$$anonfun$filter$1) is now cleaned +++
DEBUG main org.qcri.rheem.core.platform.lineage.ExecutionLineageNode - Marking ExecutionLineageNode[OperatorContext[SparkTextFileSource[Load hdfs://thor01/data/rdf/dbpedia/dbpedia-2016-04/page_links_en_uris_de.ttl]]] as executed.
INFO main org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed 1 items in 0:00:03.223 (estimated (0:00:00.010 .. 0:00:00.010, p=85.50%)).
DEBUG main org.apache.spark.storage.BlockManager - Getting local block broadcast_0
DEBUG main org.apache.spark.storage.BlockManager - Level for block broadcast_0 is StorageLevel(true, true, false, true, 1)
DEBUG main org.apache.spark.storage.BlockManager - Getting block broadcast_0 from memory
DEBUG main org.apache.spark.rdd.HadoopRDD - Creating new JobConf and caching it for later re-use
DEBUG main org.apache.hadoop.ipc.Client - The ping interval is 60000 ms.
DEBUG main org.apache.hadoop.ipc.Client - Connecting to thor01/172.16.64.55:8020
DEBUG IPC Client (872306601) connection to thor01/172.16.64.55:8020 from jonas.kemper org.apache.hadoop.ipc.Client - IPC Client (872306601) connection to thor01/172.16.64.55:8020 from jonas.kemper: starting, having connections 1
DEBUG IPC Parameter Sending Thread #0 org.apache.hadoop.ipc.Client - IPC Client (872306601) connection to thor01/172.16.64.55:8020 from jonas.kemper sending #5
DEBUG IPC Client (872306601) connection to thor01/172.16.64.55:8020 from jonas.kemper org.apache.hadoop.ipc.Client - IPC Client (872306601) connection to thor01/172.16.64.55:8020 from jonas.kemper got value #5
DEBUG main org.apache.hadoop.ipc.ProtobufRpcEngine - Call: getFileInfo took 14ms
DEBUG main org.apache.hadoop.mapred.FileInputFormat - Time taken to get FileStatuses: 17
INFO main org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
DEBUG IPC Parameter Sending Thread #0 org.apache.hadoop.ipc.Client - IPC Client (872306601) connection to thor01/172.16.64.55:8020 from jonas.kemper sending #6
DEBUG IPC Client (872306601) connection to thor01/172.16.64.55:8020 from jonas.kemper org.apache.hadoop.ipc.Client - IPC Client (872306601) connection to thor01/172.16.64.55:8020 from jonas.kemper got value #6
DEBUG main org.apache.hadoop.ipc.ProtobufRpcEngine - Call: getBlockLocations took 31ms
DEBUG main org.apache.hadoop.mapred.FileInputFormat - Total # of splits generated by getSplits: 31, TimeTaken: 71
DEBUG main org.apache.spark.util.ClosureCleaner - +++ Cleaning closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12) +++
DEBUG main org.apache.spark.util.ClosureCleaner -  + declared fields: 2
DEBUG main org.apache.spark.util.ClosureCleaner -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.serialVersionUID
DEBUG main org.apache.spark.util.ClosureCleaner -      private final org.apache.spark.rdd.RDD$$anonfun$collect$1 org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.$outer
DEBUG main org.apache.spark.util.ClosureCleaner -  + declared methods: 2
DEBUG main org.apache.spark.util.ClosureCleaner -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(java.lang.Object)
DEBUG main org.apache.spark.util.ClosureCleaner -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(scala.collection.Iterator)
DEBUG main org.apache.spark.util.ClosureCleaner -  + inner classes: 0
DEBUG main org.apache.spark.util.ClosureCleaner -  + outer classes: 2
DEBUG main org.apache.spark.util.ClosureCleaner -      org.apache.spark.rdd.RDD$$anonfun$collect$1
DEBUG main org.apache.spark.util.ClosureCleaner -      org.apache.spark.rdd.RDD
DEBUG main org.apache.spark.util.ClosureCleaner -  + outer objects: 2
DEBUG main org.apache.spark.util.ClosureCleaner -      <function0>
DEBUG main org.apache.spark.util.ClosureCleaner -      MapPartitionsRDD[2] at filter at RddChannel.java:71
DEBUG main org.apache.spark.util.ClosureCleaner -  + populating accessed fields because this is the starting closure
DEBUG main org.apache.spark.util.ClosureCleaner -  + fields accessed by starting closure: 2
DEBUG main org.apache.spark.util.ClosureCleaner -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer))
DEBUG main org.apache.spark.util.ClosureCleaner -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1))
DEBUG main org.apache.spark.util.ClosureCleaner -  + outermost object is not a closure, so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[2] at filter at RddChannel.java:71)
DEBUG main org.apache.spark.util.ClosureCleaner -  + cloning the object <function0> of class org.apache.spark.rdd.RDD$$anonfun$collect$1
DEBUG main org.apache.spark.util.ClosureCleaner -  + cleaning cloned closure <function0> recursively (org.apache.spark.rdd.RDD$$anonfun$collect$1)
DEBUG main org.apache.spark.util.ClosureCleaner - +++ Cleaning closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) +++
DEBUG main org.apache.spark.util.ClosureCleaner -  + declared fields: 2
DEBUG main org.apache.spark.util.ClosureCleaner -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1.serialVersionUID
DEBUG main org.apache.spark.util.ClosureCleaner -      private final org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.$outer
DEBUG main org.apache.spark.util.ClosureCleaner -  + declared methods: 2
DEBUG main org.apache.spark.util.ClosureCleaner -      public org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.org$apache$spark$rdd$RDD$$anonfun$$$outer()
DEBUG main org.apache.spark.util.ClosureCleaner -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1.apply()
DEBUG main org.apache.spark.util.ClosureCleaner -  + inner classes: 1
DEBUG main org.apache.spark.util.ClosureCleaner -      org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12
DEBUG main org.apache.spark.util.ClosureCleaner -  + outer classes: 1
DEBUG main org.apache.spark.util.ClosureCleaner -      org.apache.spark.rdd.RDD
DEBUG main org.apache.spark.util.ClosureCleaner -  + outer objects: 1
DEBUG main org.apache.spark.util.ClosureCleaner -      MapPartitionsRDD[2] at filter at RddChannel.java:71
DEBUG main org.apache.spark.util.ClosureCleaner -  + fields accessed by starting closure: 2
DEBUG main org.apache.spark.util.ClosureCleaner -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer))
DEBUG main org.apache.spark.util.ClosureCleaner -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1))
DEBUG main org.apache.spark.util.ClosureCleaner -  + outermost object is not a closure, so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[2] at filter at RddChannel.java:71)
DEBUG main org.apache.spark.util.ClosureCleaner -  +++ closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) is now cleaned +++
DEBUG main org.apache.spark.util.ClosureCleaner -  +++ closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12) is now cleaned +++
DEBUG main org.apache.spark.util.ClosureCleaner - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
DEBUG main org.apache.spark.util.ClosureCleaner -  + declared fields: 2
DEBUG main org.apache.spark.util.ClosureCleaner -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
DEBUG main org.apache.spark.util.ClosureCleaner -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
DEBUG main org.apache.spark.util.ClosureCleaner -  + declared methods: 2
DEBUG main org.apache.spark.util.ClosureCleaner -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
DEBUG main org.apache.spark.util.ClosureCleaner -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
DEBUG main org.apache.spark.util.ClosureCleaner -  + inner classes: 0
DEBUG main org.apache.spark.util.ClosureCleaner -  + outer classes: 0
DEBUG main org.apache.spark.util.ClosureCleaner -  + outer objects: 0
DEBUG main org.apache.spark.util.ClosureCleaner -  + populating accessed fields because this is the starting closure
DEBUG main org.apache.spark.util.ClosureCleaner -  + fields accessed by starting closure: 0
DEBUG main org.apache.spark.util.ClosureCleaner -  + there are no enclosing objects!
DEBUG main org.apache.spark.util.ClosureCleaner -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
INFO main org.apache.spark.SparkContext - Starting job: collect at SparkCollectOperator.java:43
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Got job 0 (collect at SparkCollectOperator.java:43) with 31 output partitions
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (collect at SparkCollectOperator.java:43)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
DEBUG dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - submitStage(ResultStage 0)
DEBUG dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - missing: List()
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[2] at filter at RddChannel.java:71), which has no missing parents
DEBUG dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ResultStage 0)
INFO dag-scheduler-event-loop org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 4.4 KB, free 5.1 GB)
DEBUG dag-scheduler-event-loop org.apache.spark.storage.BlockManager - Put block broadcast_1 locally took  2 ms
DEBUG dag-scheduler-event-loop org.apache.spark.storage.BlockManager - Putting block broadcast_1 without replication took  2 ms
INFO dag-scheduler-event-loop org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KB, free 5.1 GB)
INFO dispatcher-event-loop-4 org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 172.16.64.55:41258 (size: 2.5 KB, free: 5.1 GB)
DEBUG dag-scheduler-event-loop org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
DEBUG dag-scheduler-event-loop org.apache.spark.storage.BlockManager - Told master about block broadcast_1_piece0
DEBUG dag-scheduler-event-loop org.apache.spark.storage.BlockManager - Put block broadcast_1_piece0 locally took  1 ms
DEBUG dag-scheduler-event-loop org.apache.spark.storage.BlockManager - Putting block broadcast_1_piece0 without replication took  1 ms
INFO dag-scheduler-event-loop org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Submitting 31 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at filter at RddChannel.java:71)
DEBUG dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - New pending partitions: Set(0, 30, 9, 1, 2, 24, 3, 25, 4, 26, 27, 19, 20, 21, 22, 14, 23, 15, 16, 17, 18, 10, 11, 12, 13, 5, 6, 28, 7, 29, 8)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 31 tasks
DEBUG dag-scheduler-event-loop org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 0.0: 0
DEBUG dag-scheduler-event-loop org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 0.0: NODE_LOCAL, ANY
DEBUG dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
INFO dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 0, thor02.hpi.uni-potsdam.de, partition 2,NODE_LOCAL, 2457 bytes)
INFO dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 1, thor04.hpi.uni-potsdam.de, partition 0,NODE_LOCAL, 2457 bytes)
INFO dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 2, thor03.hpi.uni-potsdam.de, partition 1,NODE_LOCAL, 2457 bytes)
INFO dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 0.0 (TID 3, thor02.hpi.uni-potsdam.de, partition 6,NODE_LOCAL, 2457 bytes)
INFO dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 4, thor04.hpi.uni-potsdam.de, partition 3,NODE_LOCAL, 2457 bytes)
INFO dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 0.0 (TID 5, thor03.hpi.uni-potsdam.de, partition 4,NODE_LOCAL, 2457 bytes)
INFO dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSetManager - Starting task 11.0 in stage 0.0 (TID 6, thor02.hpi.uni-potsdam.de, partition 11,NODE_LOCAL, 2457 bytes)
INFO dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 0.0 (TID 7, thor04.hpi.uni-potsdam.de, partition 5,NODE_LOCAL, 2457 bytes)
INFO dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 0.0 (TID 8, thor03.hpi.uni-potsdam.de, partition 7,NODE_LOCAL, 2457 bytes)
INFO dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSetManager - Starting task 13.0 in stage 0.0 (TID 9, thor02.hpi.uni-potsdam.de, partition 13,NODE_LOCAL, 2457 bytes)
INFO dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 0.0 (TID 10, thor04.hpi.uni-potsdam.de, partition 8,NODE_LOCAL, 2457 bytes)
INFO dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 0.0 (TID 11, thor03.hpi.uni-potsdam.de, partition 9,NODE_LOCAL, 2457 bytes)
INFO dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSetManager - Starting task 14.0 in stage 0.0 (TID 12, thor02.hpi.uni-potsdam.de, partition 14,NODE_LOCAL, 2457 bytes)
INFO dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSetManager - Starting task 10.0 in stage 0.0 (TID 13, thor04.hpi.uni-potsdam.de, partition 10,NODE_LOCAL, 2457 bytes)
INFO dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSetManager - Starting task 12.0 in stage 0.0 (TID 14, thor03.hpi.uni-potsdam.de, partition 12,NODE_LOCAL, 2457 bytes)
INFO dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSetManager - Starting task 15.0 in stage 0.0 (TID 15, thor02.hpi.uni-potsdam.de, partition 15,NODE_LOCAL, 2457 bytes)
INFO dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSetManager - Starting task 17.0 in stage 0.0 (TID 16, thor04.hpi.uni-potsdam.de, partition 17,NODE_LOCAL, 2457 bytes)
INFO dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSetManager - Starting task 16.0 in stage 0.0 (TID 17, thor03.hpi.uni-potsdam.de, partition 16,NODE_LOCAL, 2457 bytes)
INFO dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSetManager - Starting task 18.0 in stage 0.0 (TID 18, thor02.hpi.uni-potsdam.de, partition 18,NODE_LOCAL, 2457 bytes)
INFO dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSetManager - Starting task 19.0 in stage 0.0 (TID 19, thor04.hpi.uni-potsdam.de, partition 19,NODE_LOCAL, 2457 bytes)
INFO dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSetManager - Starting task 22.0 in stage 0.0 (TID 20, thor03.hpi.uni-potsdam.de, partition 22,NODE_LOCAL, 2457 bytes)
INFO dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSetManager - Starting task 20.0 in stage 0.0 (TID 21, thor02.hpi.uni-potsdam.de, partition 20,NODE_LOCAL, 2457 bytes)
INFO dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSetManager - Starting task 21.0 in stage 0.0 (TID 22, thor04.hpi.uni-potsdam.de, partition 21,NODE_LOCAL, 2457 bytes)
INFO dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSetManager - Starting task 23.0 in stage 0.0 (TID 23, thor03.hpi.uni-potsdam.de, partition 23,NODE_LOCAL, 2457 bytes)
INFO dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSetManager - Starting task 24.0 in stage 0.0 (TID 24, thor02.hpi.uni-potsdam.de, partition 24,NODE_LOCAL, 2457 bytes)
INFO dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSetManager - Starting task 25.0 in stage 0.0 (TID 25, thor04.hpi.uni-potsdam.de, partition 25,NODE_LOCAL, 2457 bytes)
INFO dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSetManager - Starting task 26.0 in stage 0.0 (TID 26, thor03.hpi.uni-potsdam.de, partition 26,NODE_LOCAL, 2457 bytes)
INFO dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSetManager - Starting task 27.0 in stage 0.0 (TID 27, thor02.hpi.uni-potsdam.de, partition 27,NODE_LOCAL, 2457 bytes)
INFO dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSetManager - Starting task 28.0 in stage 0.0 (TID 28, thor04.hpi.uni-potsdam.de, partition 28,NODE_LOCAL, 2457 bytes)
INFO dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSetManager - Starting task 30.0 in stage 0.0 (TID 29, thor03.hpi.uni-potsdam.de, partition 30,NODE_LOCAL, 2457 bytes)
INFO dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSetManager - Starting task 29.0 in stage 0.0 (TID 30, thor02.hpi.uni-potsdam.de, partition 29,NODE_LOCAL, 2457 bytes)
DEBUG dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NODE_LOCAL, so moving to locality level ANY
DEBUG qtp2037498537-123 org.spark-project.jetty.http.HttpParser - filled 198/198
DEBUG qtp2037498537-125 org.spark-project.jetty.http.HttpParser - filled 198/198
DEBUG qtp2037498537-124 org.spark-project.jetty.http.HttpParser - filled 198/198
DEBUG qtp2037498537-123 - /jars/rheem-spark_2.11-0.3.1-SNAPSHOT.jar org.spark-project.jetty.server.Server - REQUEST /jars/rheem-spark_2.11-0.3.1-SNAPSHOT.jar on BlockingHttpConnection@c0eb76a,g=HttpGenerator{s=0,h=-1,b=-1,c=-1},p=HttpParser{s=0,l=10,c=0},r=1
DEBUG qtp2037498537-125 - /jars/rheem-spark_2.11-0.3.1-SNAPSHOT.jar org.spark-project.jetty.server.Server - REQUEST /jars/rheem-spark_2.11-0.3.1-SNAPSHOT.jar on BlockingHttpConnection@3a86a6ec,g=HttpGenerator{s=0,h=-1,b=-1,c=-1},p=HttpParser{s=0,l=10,c=0},r=1
DEBUG qtp2037498537-124 - /jars/rheem-spark_2.11-0.3.1-SNAPSHOT.jar org.spark-project.jetty.server.Server - REQUEST /jars/rheem-spark_2.11-0.3.1-SNAPSHOT.jar on BlockingHttpConnection@69129119,g=HttpGenerator{s=0,h=-1,b=-1,c=-1},p=HttpParser{s=0,l=10,c=0},r=1
DEBUG qtp2037498537-123 - /jars/rheem-spark_2.11-0.3.1-SNAPSHOT.jar org.spark-project.jetty.server.Server - RESPONSE /jars/rheem-spark_2.11-0.3.1-SNAPSHOT.jar  200 handled=true
DEBUG qtp2037498537-125 - /jars/rheem-spark_2.11-0.3.1-SNAPSHOT.jar org.spark-project.jetty.server.Server - RESPONSE /jars/rheem-spark_2.11-0.3.1-SNAPSHOT.jar  200 handled=true
DEBUG qtp2037498537-124 - /jars/rheem-spark_2.11-0.3.1-SNAPSHOT.jar org.spark-project.jetty.server.Server - RESPONSE /jars/rheem-spark_2.11-0.3.1-SNAPSHOT.jar  200 handled=true
DEBUG qtp2037498537-123 org.spark-project.jetty.http.HttpParser - filled 195/195
DEBUG qtp2037498537-123 - /jars/rheem-benchmark-1.0-SNAPSHOT.jar org.spark-project.jetty.server.Server - REQUEST /jars/rheem-benchmark-1.0-SNAPSHOT.jar on BlockingHttpConnection@c0eb76a,g=HttpGenerator{s=0,h=-1,b=-1,c=-1},p=HttpParser{s=0,l=10,c=0},r=2
DEBUG qtp2037498537-123 - /jars/rheem-benchmark-1.0-SNAPSHOT.jar org.spark-project.jetty.server.Server - RESPONSE /jars/rheem-benchmark-1.0-SNAPSHOT.jar  200 handled=true
DEBUG qtp2037498537-123 org.spark-project.jetty.http.HttpParser - filled 193/193
DEBUG qtp2037498537-123 - /jars/rheem-basic-0.3.1-SNAPSHOT.jar org.spark-project.jetty.server.Server - REQUEST /jars/rheem-basic-0.3.1-SNAPSHOT.jar on BlockingHttpConnection@c0eb76a,g=HttpGenerator{s=0,h=-1,b=-1,c=-1},p=HttpParser{s=0,l=10,c=0},r=3
DEBUG qtp2037498537-123 - /jars/rheem-basic-0.3.1-SNAPSHOT.jar org.spark-project.jetty.server.Server - RESPONSE /jars/rheem-basic-0.3.1-SNAPSHOT.jar  200 handled=true
DEBUG qtp2037498537-125 org.spark-project.jetty.http.HttpParser - filled 195/195
DEBUG qtp2037498537-125 - /jars/rheem-benchmark-1.0-SNAPSHOT.jar org.spark-project.jetty.server.Server - REQUEST /jars/rheem-benchmark-1.0-SNAPSHOT.jar on BlockingHttpConnection@3a86a6ec,g=HttpGenerator{s=0,h=-1,b=-1,c=-1},p=HttpParser{s=0,l=10,c=0},r=2
DEBUG qtp2037498537-125 - /jars/rheem-benchmark-1.0-SNAPSHOT.jar org.spark-project.jetty.server.Server - RESPONSE /jars/rheem-benchmark-1.0-SNAPSHOT.jar  200 handled=true
DEBUG qtp2037498537-123 org.spark-project.jetty.http.HttpParser - filled 192/192
DEBUG qtp2037498537-123 - /jars/rheem-core-0.3.1-SNAPSHOT.jar org.spark-project.jetty.server.Server - REQUEST /jars/rheem-core-0.3.1-SNAPSHOT.jar on BlockingHttpConnection@c0eb76a,g=HttpGenerator{s=0,h=-1,b=-1,c=-1},p=HttpParser{s=0,l=10,c=0},r=4
DEBUG qtp2037498537-124 org.spark-project.jetty.http.HttpParser - filled 195/195
DEBUG qtp2037498537-124 - /jars/rheem-benchmark-1.0-SNAPSHOT.jar org.spark-project.jetty.server.Server - REQUEST /jars/rheem-benchmark-1.0-SNAPSHOT.jar on BlockingHttpConnection@69129119,g=HttpGenerator{s=0,h=-1,b=-1,c=-1},p=HttpParser{s=0,l=10,c=0},r=2
DEBUG qtp2037498537-123 - /jars/rheem-core-0.3.1-SNAPSHOT.jar org.spark-project.jetty.server.Server - RESPONSE /jars/rheem-core-0.3.1-SNAPSHOT.jar  200 handled=true
DEBUG qtp2037498537-125 org.spark-project.jetty.http.HttpParser - filled 193/193
DEBUG qtp2037498537-125 - /jars/rheem-basic-0.3.1-SNAPSHOT.jar org.spark-project.jetty.server.Server - REQUEST /jars/rheem-basic-0.3.1-SNAPSHOT.jar on BlockingHttpConnection@3a86a6ec,g=HttpGenerator{s=0,h=-1,b=-1,c=-1},p=HttpParser{s=0,l=10,c=0},r=3
DEBUG qtp2037498537-125 - /jars/rheem-basic-0.3.1-SNAPSHOT.jar org.spark-project.jetty.server.Server - RESPONSE /jars/rheem-basic-0.3.1-SNAPSHOT.jar  200 handled=true
DEBUG qtp2037498537-124 - /jars/rheem-benchmark-1.0-SNAPSHOT.jar org.spark-project.jetty.server.Server - RESPONSE /jars/rheem-benchmark-1.0-SNAPSHOT.jar  200 handled=true
DEBUG qtp2037498537-123 org.spark-project.jetty.http.HttpParser - filled 196/196
DEBUG qtp2037498537-123 - /jars/rheem-api_2.11-0.3.1-SNAPSHOT.jar org.spark-project.jetty.server.Server - REQUEST /jars/rheem-api_2.11-0.3.1-SNAPSHOT.jar on BlockingHttpConnection@c0eb76a,g=HttpGenerator{s=0,h=-1,b=-1,c=-1},p=HttpParser{s=0,l=10,c=0},r=5
DEBUG qtp2037498537-123 - /jars/rheem-api_2.11-0.3.1-SNAPSHOT.jar org.spark-project.jetty.server.Server - RESPONSE /jars/rheem-api_2.11-0.3.1-SNAPSHOT.jar  200 handled=true
DEBUG qtp2037498537-125 org.spark-project.jetty.http.HttpParser - filled 192/192
DEBUG qtp2037498537-125 - /jars/rheem-core-0.3.1-SNAPSHOT.jar org.spark-project.jetty.server.Server - REQUEST /jars/rheem-core-0.3.1-SNAPSHOT.jar on BlockingHttpConnection@3a86a6ec,g=HttpGenerator{s=0,h=-1,b=-1,c=-1},p=HttpParser{s=0,l=10,c=0},r=4
DEBUG qtp2037498537-124 org.spark-project.jetty.http.HttpParser - filled 193/193
DEBUG qtp2037498537-124 - /jars/rheem-basic-0.3.1-SNAPSHOT.jar org.spark-project.jetty.server.Server - REQUEST /jars/rheem-basic-0.3.1-SNAPSHOT.jar on BlockingHttpConnection@69129119,g=HttpGenerator{s=0,h=-1,b=-1,c=-1},p=HttpParser{s=0,l=10,c=0},r=3
DEBUG qtp2037498537-124 - /jars/rheem-basic-0.3.1-SNAPSHOT.jar org.spark-project.jetty.server.Server - RESPONSE /jars/rheem-basic-0.3.1-SNAPSHOT.jar  200 handled=true
DEBUG qtp2037498537-125 - /jars/rheem-core-0.3.1-SNAPSHOT.jar org.spark-project.jetty.server.Server - RESPONSE /jars/rheem-core-0.3.1-SNAPSHOT.jar  200 handled=true
DEBUG qtp2037498537-124 org.spark-project.jetty.http.HttpParser - filled 192/192
DEBUG qtp2037498537-124 - /jars/rheem-core-0.3.1-SNAPSHOT.jar org.spark-project.jetty.server.Server - REQUEST /jars/rheem-core-0.3.1-SNAPSHOT.jar on BlockingHttpConnection@69129119,g=HttpGenerator{s=0,h=-1,b=-1,c=-1},p=HttpParser{s=0,l=10,c=0},r=4
DEBUG qtp2037498537-125 org.spark-project.jetty.http.HttpParser - filled 196/196
DEBUG qtp2037498537-125 - /jars/rheem-api_2.11-0.3.1-SNAPSHOT.jar org.spark-project.jetty.server.Server - REQUEST /jars/rheem-api_2.11-0.3.1-SNAPSHOT.jar on BlockingHttpConnection@3a86a6ec,g=HttpGenerator{s=0,h=-1,b=-1,c=-1},p=HttpParser{s=0,l=10,c=0},r=5
DEBUG qtp2037498537-125 - /jars/rheem-api_2.11-0.3.1-SNAPSHOT.jar org.spark-project.jetty.server.Server - RESPONSE /jars/rheem-api_2.11-0.3.1-SNAPSHOT.jar  200 handled=true
DEBUG qtp2037498537-124 - /jars/rheem-core-0.3.1-SNAPSHOT.jar org.spark-project.jetty.server.Server - RESPONSE /jars/rheem-core-0.3.1-SNAPSHOT.jar  200 handled=true
DEBUG qtp2037498537-124 org.spark-project.jetty.http.HttpParser - filled 196/196
DEBUG qtp2037498537-124 - /jars/rheem-api_2.11-0.3.1-SNAPSHOT.jar org.spark-project.jetty.server.Server - REQUEST /jars/rheem-api_2.11-0.3.1-SNAPSHOT.jar on BlockingHttpConnection@69129119,g=HttpGenerator{s=0,h=-1,b=-1,c=-1},p=HttpParser{s=0,l=10,c=0},r=5
DEBUG qtp2037498537-124 - /jars/rheem-api_2.11-0.3.1-SNAPSHOT.jar org.spark-project.jetty.server.Server - RESPONSE /jars/rheem-api_2.11-0.3.1-SNAPSHOT.jar  200 handled=true
DEBUG shuffle-server-2 org.apache.spark.storage.BlockManager - Level for block broadcast_1_piece0 is StorageLevel(true, true, false, false, 1)
DEBUG shuffle-server-2 org.apache.spark.storage.BlockManager - Getting block broadcast_1_piece0 from memory
DEBUG shuffle-server-1 org.apache.spark.storage.BlockManager - Level for block broadcast_1_piece0 is StorageLevel(true, true, false, false, 1)
DEBUG shuffle-server-1 org.apache.spark.storage.BlockManager - Getting block broadcast_1_piece0 from memory
DEBUG shuffle-server-3 org.apache.spark.storage.BlockManager - Level for block broadcast_1_piece0 is StorageLevel(true, true, false, false, 1)
DEBUG shuffle-server-3 org.apache.spark.storage.BlockManager - Getting block broadcast_1_piece0 from memory
INFO dispatcher-event-loop-3 org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on thor03.hpi.uni-potsdam.de:35741 (size: 2.5 KB, free: 14.2 GB)
INFO dispatcher-event-loop-10 org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on thor02.hpi.uni-potsdam.de:40139 (size: 2.5 KB, free: 14.2 GB)
INFO dispatcher-event-loop-16 org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on thor04.hpi.uni-potsdam.de:42815 (size: 2.5 KB, free: 14.2 GB)
INFO dispatcher-event-loop-6 org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend - Registered executor NettyRpcEndpointRef(null) (thor01.hpi.uni-potsdam.de:44962) with ID 3
DEBUG dispatcher-event-loop-6 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 31
DEBUG dispatcher-event-loop-6 org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 0.0: ANY
DEBUG shuffle-server-2 org.apache.spark.storage.BlockManager - Level for block broadcast_0_piece0 is StorageLevel(true, true, false, false, 1)
DEBUG shuffle-server-2 org.apache.spark.storage.BlockManager - Getting block broadcast_0_piece0 from memory
INFO dispatcher-event-loop-15 org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on thor03.hpi.uni-potsdam.de:35741 (size: 19.3 KB, free: 14.2 GB)
DEBUG shuffle-server-1 org.apache.spark.storage.BlockManager - Level for block broadcast_0_piece0 is StorageLevel(true, true, false, false, 1)
DEBUG shuffle-server-1 org.apache.spark.storage.BlockManager - Getting block broadcast_0_piece0 from memory
INFO dispatcher-event-loop-18 org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on thor02.hpi.uni-potsdam.de:40139 (size: 19.3 KB, free: 14.2 GB)
INFO dispatcher-event-loop-7 org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager thor01.hpi.uni-potsdam.de:38384 with 14.2 GB RAM, BlockManagerId(3, thor01.hpi.uni-potsdam.de, 38384)
INFO dispatcher-event-loop-13 org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on thor04.hpi.uni-potsdam.de:42815 (size: 19.3 KB, free: 14.2 GB)
DEBUG dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 31
DEBUG dispatcher-event-loop-5 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 31
DEBUG dispatcher-event-loop-12 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 31
DEBUG dispatcher-event-loop-0 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 31
DEBUG dispatcher-event-loop-9 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 31
DEBUG dispatcher-event-loop-3 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 31
DEBUG dispatcher-event-loop-10 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 31
DEBUG dispatcher-event-loop-16 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 31
DEBUG dispatcher-event-loop-8 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 31
DEBUG IPC Client (872306601) connection to thor01/172.16.64.55:8020 from jonas.kemper org.apache.hadoop.ipc.Client - IPC Client (872306601) connection to thor01/172.16.64.55:8020 from jonas.kemper: closed
DEBUG IPC Client (872306601) connection to thor01/172.16.64.55:8020 from jonas.kemper org.apache.hadoop.ipc.Client - IPC Client (872306601) connection to thor01/172.16.64.55:8020 from jonas.kemper: stopped, remaining connections 0
DEBUG dispatcher-event-loop-6 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 31
DEBUG qtp2037498537-123 org.spark-project.jetty.http.HttpParser - filled -1/0
DEBUG qtp2037498537-124 org.spark-project.jetty.http.HttpParser - filled -1/0
DEBUG qtp2037498537-125 org.spark-project.jetty.http.HttpParser - filled -1/0
DEBUG qtp2037498537-123 org.spark-project.jetty.server.AbstractHttpConnection - closed BlockingHttpConnection@c0eb76a,g=HttpGenerator{s=0,h=-1,b=-1,c=-1},p=HttpParser{s=0,l=0,c=-3},r=5
DEBUG qtp2037498537-124 org.spark-project.jetty.server.AbstractHttpConnection - closed BlockingHttpConnection@69129119,g=HttpGenerator{s=0,h=-1,b=-1,c=-1},p=HttpParser{s=0,l=0,c=-3},r=5
DEBUG qtp2037498537-125 org.spark-project.jetty.server.AbstractHttpConnection - closed BlockingHttpConnection@3a86a6ec,g=HttpGenerator{s=0,h=-1,b=-1,c=-1},p=HttpParser{s=0,l=0,c=-3},r=5
DEBUG dispatcher-event-loop-2 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 31
DEBUG dispatcher-event-loop-15 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 31
DEBUG dispatcher-event-loop-11 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 31
DEBUG dispatcher-event-loop-7 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 31
DEBUG dispatcher-event-loop-4 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 31
DEBUG dispatcher-event-loop-5 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 31
DEBUG dispatcher-event-loop-12 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 31
DEBUG dispatcher-event-loop-3 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 31
DEBUG dispatcher-event-loop-10 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 31
DEBUG dispatcher-event-loop-14 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 31
DEBUG dispatcher-event-loop-19 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 31
DEBUG dispatcher-event-loop-6 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 31
INFO dispatcher-event-loop-2 org.apache.spark.storage.BlockManagerInfo - Added taskresult_23 in memory on thor03.hpi.uni-potsdam.de:35741 (size: 130.4 MB, free: 14.0 GB)
DEBUG dispatcher-event-loop-15 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 30
DEBUG task-result-getter-0 org.apache.spark.scheduler.TaskResultGetter - Fetching indirect task result for TID 23
DEBUG task-result-getter-0 org.apache.spark.storage.BlockManager - Getting remote block taskresult_23 as bytes
DEBUG task-result-getter-0 org.apache.spark.storage.BlockManager - Getting remote block taskresult_23 from BlockManagerId(2, thor03.hpi.uni-potsdam.de, 35741)
DEBUG task-result-getter-0 org.apache.spark.network.client.TransportClientFactory - Creating new connection to thor03.hpi.uni-potsdam.de/172.16.64.57:35741
DEBUG task-result-getter-0 org.apache.spark.network.client.TransportClientFactory - Connection to thor03.hpi.uni-potsdam.de/172.16.64.57:35741 successful, running bootstraps...
DEBUG task-result-getter-0 org.apache.spark.network.client.TransportClientFactory - Successfully created connection to thor03.hpi.uni-potsdam.de/172.16.64.57:35741 after 2 ms (0 ms spent in bootstraps)
DEBUG shuffle-client-0 org.apache.spark.network.client.TransportClient - Sending fetch chunk request 0 to thor03.hpi.uni-potsdam.de/172.16.64.57:35741
DEBUG dispatcher-event-loop-17 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 30
INFO dispatcher-event-loop-18 org.apache.spark.storage.BlockManagerInfo - Added taskresult_20 in memory on thor03.hpi.uni-potsdam.de:35741 (size: 130.4 MB, free: 13.9 GB)
DEBUG dispatcher-event-loop-7 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 29
DEBUG task-result-getter-1 org.apache.spark.scheduler.TaskResultGetter - Fetching indirect task result for TID 20
DEBUG task-result-getter-1 org.apache.spark.storage.BlockManager - Getting remote block taskresult_20 as bytes
DEBUG task-result-getter-1 org.apache.spark.storage.BlockManager - Getting remote block taskresult_20 from BlockManagerId(2, thor03.hpi.uni-potsdam.de, 35741)
DEBUG dispatcher-event-loop-5 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 29
INFO dispatcher-event-loop-12 org.apache.spark.storage.BlockManagerInfo - Added taskresult_10 in memory on thor04.hpi.uni-potsdam.de:42815 (size: 130.4 MB, free: 14.0 GB)
DEBUG dispatcher-event-loop-0 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 28
DEBUG task-result-getter-2 org.apache.spark.scheduler.TaskResultGetter - Fetching indirect task result for TID 10
DEBUG task-result-getter-2 org.apache.spark.storage.BlockManager - Getting remote block taskresult_10 as bytes
DEBUG task-result-getter-2 org.apache.spark.storage.BlockManager - Getting remote block taskresult_10 from BlockManagerId(0, thor04.hpi.uni-potsdam.de, 42815)
DEBUG task-result-getter-2 org.apache.spark.network.client.TransportClientFactory - Creating new connection to thor04.hpi.uni-potsdam.de/172.16.64.58:42815
DEBUG task-result-getter-2 org.apache.spark.network.client.TransportClientFactory - Connection to thor04.hpi.uni-potsdam.de/172.16.64.58:42815 successful, running bootstraps...
DEBUG task-result-getter-2 org.apache.spark.network.client.TransportClientFactory - Successfully created connection to thor04.hpi.uni-potsdam.de/172.16.64.58:42815 after 2 ms (0 ms spent in bootstraps)
DEBUG shuffle-client-1 org.apache.spark.network.client.TransportClient - Sending fetch chunk request 0 to thor04.hpi.uni-potsdam.de/172.16.64.58:42815
DEBUG dispatcher-event-loop-3 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 28
DEBUG shuffle-client-1 io.netty.util.internal.Cleaner0 - java.nio.ByteBuffer.cleaner(): available
DEBUG shuffle-client-0 org.apache.spark.network.client.TransportClient - Sending fetch chunk request 0 to thor03.hpi.uni-potsdam.de/172.16.64.57:35741
INFO task-result-getter-2 org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 0.0 (TID 10) in 25771 ms on thor04.hpi.uni-potsdam.de (1/31)
INFO dispatcher-event-loop-17 org.apache.spark.storage.BlockManagerInfo - Removed taskresult_10 on thor04.hpi.uni-potsdam.de:42815 in memory (size: 130.4 MB, free: 14.2 GB)
INFO dispatcher-event-loop-17 org.apache.spark.storage.BlockManagerInfo - Added taskresult_25 in memory on thor04.hpi.uni-potsdam.de:42815 (size: 130.4 MB, free: 14.0 GB)
INFO dispatcher-event-loop-13 org.apache.spark.storage.BlockManagerInfo - Added taskresult_22 in memory on thor04.hpi.uni-potsdam.de:42815 (size: 130.4 MB, free: 13.9 GB)
INFO task-result-getter-0 org.apache.spark.scheduler.TaskSetManager - Finished task 23.0 in stage 0.0 (TID 23) in 27229 ms on thor03.hpi.uni-potsdam.de (2/31)
DEBUG dispatcher-event-loop-14 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 28
DEBUG dispatcher-event-loop-14 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 28
DEBUG dispatcher-event-loop-14 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 28
DEBUG dispatcher-event-loop-14 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 27
DEBUG task-result-getter-3 org.apache.spark.scheduler.TaskResultGetter - Fetching indirect task result for TID 25
DEBUG dispatcher-event-loop-14 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 27
DEBUG task-result-getter-3 org.apache.spark.storage.BlockManager - Getting remote block taskresult_25 as bytes
DEBUG dispatcher-event-loop-14 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 26
DEBUG task-result-getter-2 org.apache.spark.scheduler.TaskResultGetter - Fetching indirect task result for TID 22
DEBUG task-result-getter-3 org.apache.spark.storage.BlockManager - Getting remote block taskresult_25 from BlockManagerId(0, thor04.hpi.uni-potsdam.de, 42815)
DEBUG task-result-getter-2 org.apache.spark.storage.BlockManager - Getting remote block taskresult_22 as bytes
DEBUG task-result-getter-2 org.apache.spark.storage.BlockManager - Getting remote block taskresult_22 from BlockManagerId(0, thor04.hpi.uni-potsdam.de, 42815)
DEBUG shuffle-client-1 org.apache.spark.network.client.TransportClient - Sending fetch chunk request 0 to thor04.hpi.uni-potsdam.de/172.16.64.58:42815
DEBUG shuffle-client-1 org.apache.spark.network.client.TransportClient - Sending fetch chunk request 0 to thor04.hpi.uni-potsdam.de/172.16.64.58:42815
INFO dispatcher-event-loop-3 org.apache.spark.storage.BlockManagerInfo - Added taskresult_4 in memory on thor04.hpi.uni-potsdam.de:42815 (size: 130.4 MB, free: 13.8 GB)
DEBUG dispatcher-event-loop-10 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 25
DEBUG task-result-getter-0 org.apache.spark.scheduler.TaskResultGetter - Fetching indirect task result for TID 4
DEBUG task-result-getter-0 org.apache.spark.storage.BlockManager - Getting remote block taskresult_4 as bytes
DEBUG task-result-getter-0 org.apache.spark.storage.BlockManager - Getting remote block taskresult_4 from BlockManagerId(0, thor04.hpi.uni-potsdam.de, 42815)
INFO dispatcher-event-loop-8 org.apache.spark.storage.BlockManagerInfo - Added taskresult_13 in memory on thor04.hpi.uni-potsdam.de:42815 (size: 130.4 MB, free: 13.6 GB)
DEBUG dispatcher-event-loop-19 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 24
INFO dispatcher-event-loop-6 org.apache.spark.storage.BlockManagerInfo - Added taskresult_28 in memory on thor04.hpi.uni-potsdam.de:42815 (size: 130.4 MB, free: 13.5 GB)
DEBUG dispatcher-event-loop-2 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 23
DEBUG dispatcher-event-loop-15 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 23
DEBUG dispatcher-event-loop-11 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 22
INFO dispatcher-event-loop-18 org.apache.spark.storage.BlockManagerInfo - Removed taskresult_23 on thor03.hpi.uni-potsdam.de:35741 in memory (size: 130.4 MB, free: 14.0 GB)
INFO dispatcher-event-loop-18 org.apache.spark.storage.BlockManagerInfo - Added taskresult_14 in memory on thor03.hpi.uni-potsdam.de:35741 (size: 130.4 MB, free: 13.9 GB)
INFO dispatcher-event-loop-18 org.apache.spark.storage.BlockManagerInfo - Added taskresult_17 in memory on thor03.hpi.uni-potsdam.de:35741 (size: 130.4 MB, free: 13.8 GB)
INFO dispatcher-event-loop-18 org.apache.spark.storage.BlockManagerInfo - Added taskresult_5 in memory on thor03.hpi.uni-potsdam.de:35741 (size: 130.4 MB, free: 13.6 GB)
INFO dispatcher-event-loop-18 org.apache.spark.storage.BlockManagerInfo - Added taskresult_26 in memory on thor03.hpi.uni-potsdam.de:35741 (size: 130.4 MB, free: 13.5 GB)
INFO dispatcher-event-loop-18 org.apache.spark.storage.BlockManagerInfo - Added taskresult_11 in memory on thor03.hpi.uni-potsdam.de:35741 (size: 130.4 MB, free: 13.4 GB)
INFO dispatcher-event-loop-18 org.apache.spark.storage.BlockManagerInfo - Added taskresult_8 in memory on thor03.hpi.uni-potsdam.de:35741 (size: 130.4 MB, free: 13.3 GB)
INFO dispatcher-event-loop-18 org.apache.spark.storage.BlockManagerInfo - Added taskresult_2 in memory on thor03.hpi.uni-potsdam.de:35741 (size: 130.5 MB, free: 13.1 GB)
DEBUG dispatcher-event-loop-12 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 21
DEBUG dispatcher-event-loop-12 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 20
DEBUG dispatcher-event-loop-9 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 19
DEBUG dispatcher-event-loop-3 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 18
DEBUG dispatcher-event-loop-3 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 17
DEBUG dispatcher-event-loop-3 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 16
DEBUG dispatcher-event-loop-3 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 15
DEBUG dispatcher-event-loop-19 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 15
INFO dispatcher-event-loop-6 org.apache.spark.storage.BlockManagerInfo - Added taskresult_12 in memory on thor02.hpi.uni-potsdam.de:40139 (size: 130.4 MB, free: 14.0 GB)
INFO dispatcher-event-loop-6 org.apache.spark.storage.BlockManagerInfo - Added taskresult_16 in memory on thor04.hpi.uni-potsdam.de:42815 (size: 130.4 MB, free: 13.4 GB)
INFO dispatcher-event-loop-6 org.apache.spark.storage.BlockManagerInfo - Added taskresult_15 in memory on thor02.hpi.uni-potsdam.de:40139 (size: 130.4 MB, free: 13.9 GB)
DEBUG dispatcher-event-loop-11 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 14
DEBUG dispatcher-event-loop-17 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 13
DEBUG dispatcher-event-loop-7 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 12
INFO dispatcher-event-loop-4 org.apache.spark.storage.BlockManagerInfo - Added taskresult_7 in memory on thor04.hpi.uni-potsdam.de:42815 (size: 130.4 MB, free: 13.3 GB)
INFO dispatcher-event-loop-4 org.apache.spark.storage.BlockManagerInfo - Added taskresult_19 in memory on thor04.hpi.uni-potsdam.de:42815 (size: 130.4 MB, free: 13.1 GB)
INFO dispatcher-event-loop-4 org.apache.spark.storage.BlockManagerInfo - Added taskresult_1 in memory on thor04.hpi.uni-potsdam.de:42815 (size: 130.5 MB, free: 13.0 GB)
DEBUG dispatcher-event-loop-14 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 11
DEBUG dispatcher-event-loop-14 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 10
DEBUG dispatcher-event-loop-14 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 9
INFO dispatcher-event-loop-12 org.apache.spark.storage.BlockManagerInfo - Removed taskresult_20 on thor03.hpi.uni-potsdam.de:35741 in memory (size: 130.4 MB, free: 13.3 GB)
INFO dispatcher-event-loop-9 org.apache.spark.storage.BlockManagerInfo - Added taskresult_0 in memory on thor02.hpi.uni-potsdam.de:40139 (size: 130.4 MB, free: 13.8 GB)
INFO dispatcher-event-loop-16 org.apache.spark.storage.BlockManagerInfo - Added taskresult_9 in memory on thor02.hpi.uni-potsdam.de:40139 (size: 130.4 MB, free: 13.6 GB)
INFO dispatcher-event-loop-2 org.apache.spark.storage.BlockManagerInfo - Added taskresult_6 in memory on thor02.hpi.uni-potsdam.de:40139 (size: 130.4 MB, free: 13.5 GB)
INFO dispatcher-event-loop-19 org.apache.spark.storage.BlockManagerInfo - Added taskresult_27 in memory on thor02.hpi.uni-potsdam.de:40139 (size: 130.4 MB, free: 13.4 GB)
INFO dispatcher-event-loop-11 org.apache.spark.storage.BlockManagerInfo - Removed taskresult_25 on thor04.hpi.uni-potsdam.de:42815 in memory (size: 130.4 MB, free: 13.1 GB)
INFO task-result-getter-1 org.apache.spark.scheduler.TaskSetManager - Finished task 22.0 in stage 0.0 (TID 20) in 31182 ms on thor03.hpi.uni-potsdam.de (3/31)
INFO dispatcher-event-loop-17 org.apache.spark.storage.BlockManagerInfo - Added taskresult_30 in memory on thor02.hpi.uni-potsdam.de:40139 (size: 130.4 MB, free: 13.3 GB)
INFO dispatcher-event-loop-4 org.apache.spark.storage.BlockManagerInfo - Added taskresult_21 in memory on thor02.hpi.uni-potsdam.de:40139 (size: 130.4 MB, free: 13.1 GB)
INFO dispatcher-event-loop-0 org.apache.spark.storage.BlockManagerInfo - Added taskresult_18 in memory on thor02.hpi.uni-potsdam.de:40139 (size: 130.4 MB, free: 13.0 GB)
INFO dispatcher-event-loop-0 org.apache.spark.storage.BlockManagerInfo - Added taskresult_3 in memory on thor02.hpi.uni-potsdam.de:40139 (size: 130.4 MB, free: 12.9 GB)
INFO dispatcher-event-loop-0 org.apache.spark.storage.BlockManagerInfo - Added taskresult_24 in memory on thor02.hpi.uni-potsdam.de:40139 (size: 130.4 MB, free: 12.8 GB)
INFO task-result-getter-3 org.apache.spark.scheduler.TaskSetManager - Finished task 25.0 in stage 0.0 (TID 25) in 33085 ms on thor04.hpi.uni-potsdam.de (4/31)
DEBUG task-result-getter-3 org.apache.spark.scheduler.TaskResultGetter - Fetching indirect task result for TID 28
DEBUG dispatcher-event-loop-10 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 8
DEBUG task-result-getter-1 org.apache.spark.scheduler.TaskResultGetter - Fetching indirect task result for TID 13
DEBUG dispatcher-event-loop-10 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 7
DEBUG task-result-getter-3 org.apache.spark.storage.BlockManager - Getting remote block taskresult_28 as bytes
DEBUG task-result-getter-1 org.apache.spark.storage.BlockManager - Getting remote block taskresult_13 as bytes
DEBUG dispatcher-event-loop-10 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 7
DEBUG task-result-getter-3 org.apache.spark.storage.BlockManager - Getting remote block taskresult_28 from BlockManagerId(0, thor04.hpi.uni-potsdam.de, 42815)
DEBUG task-result-getter-1 org.apache.spark.storage.BlockManager - Getting remote block taskresult_13 from BlockManagerId(0, thor04.hpi.uni-potsdam.de, 42815)
DEBUG dispatcher-event-loop-10 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 7
DEBUG dispatcher-event-loop-10 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 6
DEBUG dispatcher-event-loop-10 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 5
DEBUG dispatcher-event-loop-10 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 4
DEBUG dispatcher-event-loop-10 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 3
DEBUG dispatcher-event-loop-10 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 3
DEBUG dispatcher-event-loop-10 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 2
DEBUG dispatcher-event-loop-10 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 1
DEBUG dispatcher-event-loop-10 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG shuffle-client-1 org.apache.spark.network.client.TransportClient - Sending fetch chunk request 0 to thor04.hpi.uni-potsdam.de/172.16.64.58:42815
DEBUG shuffle-client-1 org.apache.spark.network.client.TransportClient - Sending fetch chunk request 0 to thor04.hpi.uni-potsdam.de/172.16.64.58:42815
DEBUG shuffle-client-1 org.apache.spark.network.client.TransportClient - Sending fetch chunk request 0 to thor04.hpi.uni-potsdam.de/172.16.64.58:42815
INFO dispatcher-event-loop-11 org.apache.spark.storage.BlockManagerInfo - Removed taskresult_22 on thor04.hpi.uni-potsdam.de:42815 in memory (size: 130.4 MB, free: 13.3 GB)
INFO task-result-getter-2 org.apache.spark.scheduler.TaskSetManager - Finished task 21.0 in stage 0.0 (TID 22) in 34623 ms on thor04.hpi.uni-potsdam.de (5/31)
DEBUG dispatcher-event-loop-17 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG dispatcher-event-loop-17 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG dispatcher-event-loop-4 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
INFO dispatcher-event-loop-12 org.apache.spark.storage.BlockManagerInfo - Removed taskresult_4 on thor04.hpi.uni-potsdam.de:42815 in memory (size: 130.4 MB, free: 13.4 GB)
INFO task-result-getter-0 org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 0.0 (TID 4) in 37366 ms on thor04.hpi.uni-potsdam.de (6/31)
INFO task-result-getter-2 org.apache.spark.scheduler.TaskSetManager - Finished task 30.0 in stage 0.0 (TID 29) in 38536 ms on thor03.hpi.uni-potsdam.de (7/31)
DEBUG dispatcher-event-loop-9 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG task-result-getter-0 org.apache.spark.scheduler.TaskResultGetter - Fetching indirect task result for TID 17
DEBUG task-result-getter-2 org.apache.spark.scheduler.TaskResultGetter - Fetching indirect task result for TID 14
DEBUG task-result-getter-0 org.apache.spark.storage.BlockManager - Getting remote block taskresult_17 as bytes
DEBUG task-result-getter-2 org.apache.spark.storage.BlockManager - Getting remote block taskresult_14 as bytes
DEBUG task-result-getter-0 org.apache.spark.storage.BlockManager - Getting remote block taskresult_17 from BlockManagerId(2, thor03.hpi.uni-potsdam.de, 35741)
DEBUG task-result-getter-2 org.apache.spark.storage.BlockManager - Getting remote block taskresult_14 from BlockManagerId(2, thor03.hpi.uni-potsdam.de, 35741)
DEBUG shuffle-client-0 org.apache.spark.network.client.TransportClient - Sending fetch chunk request 0 to thor03.hpi.uni-potsdam.de/172.16.64.57:35741
DEBUG shuffle-client-0 org.apache.spark.network.client.TransportClient - Sending fetch chunk request 0 to thor03.hpi.uni-potsdam.de/172.16.64.57:35741
DEBUG dispatcher-event-loop-8 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
INFO dispatcher-event-loop-19 org.apache.spark.storage.BlockManagerInfo - Removed taskresult_28 on thor04.hpi.uni-potsdam.de:42815 in memory (size: 130.4 MB, free: 13.5 GB)
INFO task-result-getter-3 org.apache.spark.scheduler.TaskSetManager - Finished task 28.0 in stage 0.0 (TID 28) in 39087 ms on thor04.hpi.uni-potsdam.de (8/31)
DEBUG dispatcher-event-loop-10 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG task-result-getter-3 org.apache.spark.scheduler.TaskResultGetter - Fetching indirect task result for TID 5
DEBUG dispatcher-event-loop-10 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG task-result-getter-3 org.apache.spark.storage.BlockManager - Getting remote block taskresult_5 as bytes
DEBUG task-result-getter-3 org.apache.spark.storage.BlockManager - Getting remote block taskresult_5 from BlockManagerId(2, thor03.hpi.uni-potsdam.de, 35741)
DEBUG dispatcher-event-loop-7 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
INFO dispatcher-event-loop-1 org.apache.spark.storage.BlockManagerInfo - Removed taskresult_17 on thor03.hpi.uni-potsdam.de:35741 in memory (size: 130.4 MB, free: 13.4 GB)
INFO task-result-getter-0 org.apache.spark.scheduler.TaskSetManager - Finished task 16.0 in stage 0.0 (TID 17) in 42152 ms on thor03.hpi.uni-potsdam.de (9/31)
DEBUG dispatcher-event-loop-17 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG task-result-getter-0 org.apache.spark.scheduler.TaskResultGetter - Fetching indirect task result for TID 26
DEBUG task-result-getter-0 org.apache.spark.storage.BlockManager - Getting remote block taskresult_26 as bytes
DEBUG task-result-getter-0 org.apache.spark.storage.BlockManager - Getting remote block taskresult_26 from BlockManagerId(2, thor03.hpi.uni-potsdam.de, 35741)
DEBUG dispatcher-event-loop-14 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG shuffle-client-0 org.apache.spark.network.client.TransportClient - Sending fetch chunk request 0 to thor03.hpi.uni-potsdam.de/172.16.64.57:35741
DEBUG shuffle-client-0 org.apache.spark.network.client.TransportClient - Sending fetch chunk request 0 to thor03.hpi.uni-potsdam.de/172.16.64.57:35741
INFO dispatcher-event-loop-9 org.apache.spark.storage.BlockManagerInfo - Removed taskresult_14 on thor03.hpi.uni-potsdam.de:35741 in memory (size: 130.4 MB, free: 13.5 GB)
INFO task-result-getter-2 org.apache.spark.scheduler.TaskSetManager - Finished task 12.0 in stage 0.0 (TID 14) in 44158 ms on thor03.hpi.uni-potsdam.de (10/31)
DEBUG dispatcher-event-loop-16 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG task-result-getter-2 org.apache.spark.scheduler.TaskResultGetter - Fetching indirect task result for TID 8
DEBUG task-result-getter-2 org.apache.spark.storage.BlockManager - Getting remote block taskresult_8 as bytes
DEBUG task-result-getter-2 org.apache.spark.storage.BlockManager - Getting remote block taskresult_8 from BlockManagerId(2, thor03.hpi.uni-potsdam.de, 35741)
DEBUG dispatcher-event-loop-2 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
INFO dispatcher-event-loop-19 org.apache.spark.storage.BlockManagerInfo - Removed taskresult_5 on thor03.hpi.uni-potsdam.de:35741 in memory (size: 130.4 MB, free: 13.6 GB)
INFO dispatcher-event-loop-7 org.apache.spark.storage.BlockManagerInfo - Removed taskresult_13 on thor04.hpi.uni-potsdam.de:42815 in memory (size: 130.4 MB, free: 13.6 GB)
INFO task-result-getter-3 org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 0.0 (TID 5) in 46179 ms on thor03.hpi.uni-potsdam.de (11/31)
DEBUG shuffle-client-0 org.apache.spark.network.client.TransportClient - Sending fetch chunk request 0 to thor03.hpi.uni-potsdam.de/172.16.64.57:35741
INFO dispatcher-event-loop-5 org.apache.spark.storage.BlockManagerInfo - Removed taskresult_26 on thor03.hpi.uni-potsdam.de:35741 in memory (size: 130.4 MB, free: 13.8 GB)
INFO task-result-getter-1 org.apache.spark.scheduler.TaskSetManager - Finished task 10.0 in stage 0.0 (TID 13) in 48464 ms on thor04.hpi.uni-potsdam.de (12/31)
DEBUG dispatcher-event-loop-6 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG task-result-getter-1 org.apache.spark.scheduler.TaskResultGetter - Fetching indirect task result for TID 2
INFO task-result-getter-0 org.apache.spark.scheduler.TaskSetManager - Finished task 26.0 in stage 0.0 (TID 26) in 49422 ms on thor03.hpi.uni-potsdam.de (13/31)
DEBUG task-result-getter-3 org.apache.spark.scheduler.TaskResultGetter - Fetching indirect task result for TID 11
DEBUG task-result-getter-1 org.apache.spark.storage.BlockManager - Getting remote block taskresult_2 as bytes
DEBUG dispatcher-event-loop-6 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG task-result-getter-1 org.apache.spark.storage.BlockManager - Getting remote block taskresult_2 from BlockManagerId(2, thor03.hpi.uni-potsdam.de, 35741)
DEBUG task-result-getter-0 org.apache.spark.scheduler.TaskResultGetter - Fetching indirect task result for TID 12
DEBUG task-result-getter-3 org.apache.spark.storage.BlockManager - Getting remote block taskresult_11 as bytes
DEBUG task-result-getter-0 org.apache.spark.storage.BlockManager - Getting remote block taskresult_12 as bytes
DEBUG dispatcher-event-loop-6 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG task-result-getter-3 org.apache.spark.storage.BlockManager - Getting remote block taskresult_11 from BlockManagerId(2, thor03.hpi.uni-potsdam.de, 35741)
DEBUG dispatcher-event-loop-6 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG task-result-getter-0 org.apache.spark.storage.BlockManager - Getting remote block taskresult_12 from BlockManagerId(1, thor02.hpi.uni-potsdam.de, 40139)
DEBUG task-result-getter-0 org.apache.spark.network.client.TransportClientFactory - Creating new connection to thor02.hpi.uni-potsdam.de/172.16.64.56:40139
DEBUG task-result-getter-0 org.apache.spark.network.client.TransportClientFactory - Connection to thor02.hpi.uni-potsdam.de/172.16.64.56:40139 successful, running bootstraps...
DEBUG task-result-getter-0 org.apache.spark.network.client.TransportClientFactory - Successfully created connection to thor02.hpi.uni-potsdam.de/172.16.64.56:40139 after 20 ms (0 ms spent in bootstraps)
DEBUG dispatcher-event-loop-16 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG dispatcher-event-loop-16 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG shuffle-client-0 org.apache.spark.network.client.TransportClient - Sending fetch chunk request 0 to thor03.hpi.uni-potsdam.de/172.16.64.57:35741
DEBUG shuffle-client-0 org.apache.spark.network.client.TransportClient - Sending fetch chunk request 0 to thor03.hpi.uni-potsdam.de/172.16.64.57:35741
DEBUG shuffle-client-2 org.apache.spark.network.client.TransportClient - Sending fetch chunk request 0 to thor02.hpi.uni-potsdam.de/172.16.64.56:40139
INFO dispatcher-event-loop-15 org.apache.spark.storage.BlockManagerInfo - Removed taskresult_8 on thor03.hpi.uni-potsdam.de:35741 in memory (size: 130.4 MB, free: 13.9 GB)
INFO task-result-getter-2 org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 0.0 (TID 8) in 52301 ms on thor03.hpi.uni-potsdam.de (14/31)
DEBUG dispatcher-event-loop-10 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG task-result-getter-2 org.apache.spark.scheduler.TaskResultGetter - Fetching indirect task result for TID 15
DEBUG task-result-getter-2 org.apache.spark.storage.BlockManager - Getting remote block taskresult_15 as bytes
DEBUG task-result-getter-2 org.apache.spark.storage.BlockManager - Getting remote block taskresult_15 from BlockManagerId(1, thor02.hpi.uni-potsdam.de, 40139)
DEBUG dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG shuffle-client-2 org.apache.spark.network.client.TransportClient - Sending fetch chunk request 0 to thor02.hpi.uni-potsdam.de/172.16.64.56:40139
INFO dispatcher-event-loop-4 org.apache.spark.storage.BlockManagerInfo - Removed taskresult_12 on thor02.hpi.uni-potsdam.de:40139 in memory (size: 130.4 MB, free: 12.9 GB)
INFO task-result-getter-0 org.apache.spark.scheduler.TaskSetManager - Finished task 14.0 in stage 0.0 (TID 12) in 54209 ms on thor02.hpi.uni-potsdam.de (15/31)
DEBUG dispatcher-event-loop-5 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG task-result-getter-0 org.apache.spark.scheduler.TaskResultGetter - Fetching indirect task result for TID 16
DEBUG task-result-getter-0 org.apache.spark.storage.BlockManager - Getting remote block taskresult_16 as bytes
DEBUG task-result-getter-0 org.apache.spark.storage.BlockManager - Getting remote block taskresult_16 from BlockManagerId(0, thor04.hpi.uni-potsdam.de, 42815)
DEBUG shuffle-client-1 org.apache.spark.network.client.TransportClient - Sending fetch chunk request 0 to thor04.hpi.uni-potsdam.de/172.16.64.58:42815
DEBUG dispatcher-event-loop-9 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
INFO dispatcher-event-loop-0 org.apache.spark.storage.BlockManagerInfo - Removed taskresult_2 on thor03.hpi.uni-potsdam.de:35741 in memory (size: 130.5 MB, free: 14.0 GB)
INFO task-result-getter-1 org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 2) in 56486 ms on thor03.hpi.uni-potsdam.de (16/31)
DEBUG dispatcher-event-loop-6 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG task-result-getter-1 org.apache.spark.scheduler.TaskResultGetter - Fetching indirect task result for TID 7
DEBUG dispatcher-event-loop-6 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG task-result-getter-1 org.apache.spark.storage.BlockManager - Getting remote block taskresult_7 as bytes
DEBUG task-result-getter-1 org.apache.spark.storage.BlockManager - Getting remote block taskresult_7 from BlockManagerId(0, thor04.hpi.uni-potsdam.de, 42815)
DEBUG dispatcher-event-loop-19 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
INFO dispatcher-event-loop-13 org.apache.spark.storage.BlockManagerInfo - Removed taskresult_15 on thor02.hpi.uni-potsdam.de:40139 in memory (size: 130.4 MB, free: 13.0 GB)
DEBUG shuffle-client-1 org.apache.spark.network.client.TransportClient - Sending fetch chunk request 0 to thor04.hpi.uni-potsdam.de/172.16.64.58:42815
INFO dispatcher-event-loop-18 org.apache.spark.storage.BlockManagerInfo - Removed taskresult_16 on thor04.hpi.uni-potsdam.de:42815 in memory (size: 130.4 MB, free: 13.8 GB)
INFO task-result-getter-2 org.apache.spark.scheduler.TaskSetManager - Finished task 15.0 in stage 0.0 (TID 15) in 59369 ms on thor02.hpi.uni-potsdam.de (17/31)
INFO dispatcher-event-loop-12 org.apache.spark.storage.BlockManagerInfo - Removed taskresult_11 on thor03.hpi.uni-potsdam.de:35741 in memory (size: 130.4 MB, free: 14.2 GB)
INFO task-result-getter-0 org.apache.spark.scheduler.TaskSetManager - Finished task 17.0 in stage 0.0 (TID 16) in 61278 ms on thor04.hpi.uni-potsdam.de (18/31)
DEBUG dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG task-result-getter-0 org.apache.spark.scheduler.TaskResultGetter - Fetching indirect task result for TID 1
DEBUG dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
INFO dispatcher-event-loop-17 org.apache.spark.storage.BlockManagerInfo - Removed taskresult_7 on thor04.hpi.uni-potsdam.de:42815 in memory (size: 130.4 MB, free: 13.9 GB)
INFO task-result-getter-3 org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 0.0 (TID 11) in 64745 ms on thor03.hpi.uni-potsdam.de (19/31)
DEBUG task-result-getter-2 org.apache.spark.scheduler.TaskResultGetter - Fetching indirect task result for TID 19
INFO task-result-getter-1 org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 0.0 (TID 7) in 71868 ms on thor04.hpi.uni-potsdam.de (20/31)
DEBUG dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG task-result-getter-0 org.apache.spark.storage.BlockManager - Getting remote block taskresult_1 as bytes
DEBUG task-result-getter-1 org.apache.spark.scheduler.TaskResultGetter - Fetching indirect task result for TID 9
DEBUG task-result-getter-2 org.apache.spark.storage.BlockManager - Getting remote block taskresult_19 as bytes
DEBUG dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG task-result-getter-3 org.apache.spark.scheduler.TaskResultGetter - Fetching indirect task result for TID 0
DEBUG task-result-getter-2 org.apache.spark.storage.BlockManager - Getting remote block taskresult_19 from BlockManagerId(0, thor04.hpi.uni-potsdam.de, 42815)
DEBUG task-result-getter-0 org.apache.spark.storage.BlockManager - Getting remote block taskresult_1 from BlockManagerId(0, thor04.hpi.uni-potsdam.de, 42815)
DEBUG dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG task-result-getter-3 org.apache.spark.storage.BlockManager - Getting remote block taskresult_0 as bytes
DEBUG task-result-getter-1 org.apache.spark.storage.BlockManager - Getting remote block taskresult_9 as bytes
DEBUG dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG task-result-getter-1 org.apache.spark.storage.BlockManager - Getting remote block taskresult_9 from BlockManagerId(1, thor02.hpi.uni-potsdam.de, 40139)
DEBUG task-result-getter-3 org.apache.spark.storage.BlockManager - Getting remote block taskresult_0 from BlockManagerId(1, thor02.hpi.uni-potsdam.de, 40139)
DEBUG dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG shuffle-client-1 org.apache.spark.network.client.TransportClient - Sending fetch chunk request 0 to thor04.hpi.uni-potsdam.de/172.16.64.58:42815
DEBUG dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG shuffle-client-1 org.apache.spark.network.client.TransportClient - Sending fetch chunk request 0 to thor04.hpi.uni-potsdam.de/172.16.64.58:42815
DEBUG dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG shuffle-client-2 org.apache.spark.network.client.TransportClient - Sending fetch chunk request 0 to thor02.hpi.uni-potsdam.de/172.16.64.56:40139
DEBUG dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG shuffle-client-2 org.apache.spark.network.client.TransportClient - Sending fetch chunk request 0 to thor02.hpi.uni-potsdam.de/172.16.64.56:40139
DEBUG dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG dispatcher-event-loop-2 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG dispatcher-event-loop-19 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG dispatcher-event-loop-15 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
WARN dispatcher-event-loop-8 org.apache.spark.HeartbeatReceiver - Removing executor 2 with no recent heartbeats: 179080 ms exceeds timeout 120000 ms
DEBUG dispatcher-event-loop-15 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
WARN shuffle-server-0 org.apache.spark.network.server.TransportChannelHandler - Exception in connection from thor01.hpi.uni-potsdam.de/172.16.64.55:44962
java.lang.OutOfMemoryError: Java heap space
	at java.lang.Class.getDeclaredMethods0(Native Method)
	at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)
	at java.lang.Class.getDeclaredMethod(Class.java:2128)
	at java.io.ObjectStreamClass.getInheritableMethod(ObjectStreamClass.java:1442)
	at java.io.ObjectStreamClass.access$2200(ObjectStreamClass.java:72)
	at java.io.ObjectStreamClass$2.run(ObjectStreamClass.java:508)
	at java.io.ObjectStreamClass$2.run(ObjectStreamClass.java:472)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.io.ObjectStreamClass.<init>(ObjectStreamClass.java:472)
	at java.io.ObjectStreamClass.lookup(ObjectStreamClass.java:369)
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:598)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1630)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1521)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1781)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:373)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:258)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:310)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:256)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:588)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:570)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:149)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:102)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:104)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:51)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
ERROR dispatcher-event-loop-8 org.apache.spark.scheduler.TaskSchedulerImpl - Lost executor 2 on thor03.hpi.uni-potsdam.de: Executor heartbeat timed out after 179080 ms
WARN shuffle-client-1 org.apache.spark.network.server.TransportChannelHandler - Exception in connection from thor04.hpi.uni-potsdam.de/172.16.64.58:42815
java.lang.OutOfMemoryError: Java heap space
	at java.nio.DirectByteBuffer.slice(DirectByteBuffer.java:213)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.nioBuffer(PooledUnsafeDirectByteBuf.java:348)
	at io.netty.buffer.SlicedByteBuf.nioBuffer(SlicedByteBuf.java:271)
	at io.netty.buffer.CompositeByteBuf.nioBuffers(CompositeByteBuf.java:1189)
	at io.netty.buffer.CompositeByteBuf.nioBuffer(CompositeByteBuf.java:1161)
	at io.netty.buffer.AbstractDerivedByteBuf.nioBuffer(AbstractDerivedByteBuf.java:73)
	at io.netty.buffer.AbstractByteBuf.nioBuffer(AbstractByteBuf.java:939)
	at org.apache.spark.network.buffer.NettyManagedBuffer.nioByteBuffer(NettyManagedBuffer.java:45)
	at org.apache.spark.network.BlockTransferService$$anon$1.onBlockFetchSuccess(BlockTransferService.scala:96)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$RetryingBlockFetchListener.onBlockFetchSuccess(RetryingBlockFetcher.java:206)
	at org.apache.spark.network.shuffle.OneForOneBlockFetcher$ChunkCallback.onSuccess(OneForOneBlockFetcher.java:72)
	at org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:150)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:106)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:51)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:86)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
WARN shuffle-server-4 org.apache.spark.network.server.TransportChannelHandler - Exception in connection from thor04.hpi.uni-potsdam.de/172.16.64.58:33750
java.lang.OutOfMemoryError: Java heap space
	at java.lang.Class.getDeclaredMethods0(Native Method)
	at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)
	at java.lang.Class.getDeclaredMethod(Class.java:2128)
	at java.io.ObjectStreamClass.getInheritableMethod(ObjectStreamClass.java:1442)
	at java.io.ObjectStreamClass.access$2200(ObjectStreamClass.java:72)
	at java.io.ObjectStreamClass$2.run(ObjectStreamClass.java:508)
	at java.io.ObjectStreamClass$2.run(ObjectStreamClass.java:472)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.io.ObjectStreamClass.<init>(ObjectStreamClass.java:472)
	at java.io.ObjectStreamClass.lookup(ObjectStreamClass.java:369)
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:598)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1630)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1521)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1781)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:373)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:258)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:310)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:256)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:588)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:570)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:149)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:102)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:104)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:51)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
WARN shuffle-server-6 org.apache.spark.network.server.TransportChannelHandler - Exception in connection from thor02.hpi.uni-potsdam.de/172.16.64.56:41478
java.lang.OutOfMemoryError: Java heap space
	at java.lang.Class.getDeclaredMethods0(Native Method)
	at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)
	at java.lang.Class.getDeclaredMethod(Class.java:2128)
	at java.io.ObjectStreamClass.getInheritableMethod(ObjectStreamClass.java:1442)
	at java.io.ObjectStreamClass.access$2200(ObjectStreamClass.java:72)
	at java.io.ObjectStreamClass$2.run(ObjectStreamClass.java:508)
	at java.io.ObjectStreamClass$2.run(ObjectStreamClass.java:472)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.io.ObjectStreamClass.<init>(ObjectStreamClass.java:472)
	at java.io.ObjectStreamClass.lookup(ObjectStreamClass.java:369)
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:598)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1630)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1521)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1781)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:373)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:258)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:310)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:256)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:588)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:570)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:149)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:102)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:104)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:51)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
WARN shuffle-server-5 org.apache.spark.network.server.TransportChannelHandler - Exception in connection from thor03.hpi.uni-potsdam.de/172.16.64.57:42492
java.lang.OutOfMemoryError: Java heap space
	at java.lang.Class.getDeclaredMethods0(Native Method)
	at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)
	at java.lang.Class.getDeclaredMethod(Class.java:2128)
	at java.io.ObjectStreamClass.getInheritableMethod(ObjectStreamClass.java:1442)
	at java.io.ObjectStreamClass.access$2200(ObjectStreamClass.java:72)
	at java.io.ObjectStreamClass$2.run(ObjectStreamClass.java:508)
	at java.io.ObjectStreamClass$2.run(ObjectStreamClass.java:472)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.io.ObjectStreamClass.<init>(ObjectStreamClass.java:472)
	at java.io.ObjectStreamClass.lookup(ObjectStreamClass.java:369)
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:598)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1630)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1521)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1781)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:373)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:258)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:310)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:256)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:588)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:570)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:149)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:102)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:104)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:51)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
ERROR shuffle-client-1 org.apache.spark.network.client.TransportResponseHandler - Still have 1 requests outstanding when connection from thor04.hpi.uni-potsdam.de/172.16.64.58:42815 is closed
ERROR shuffle-client-1 org.apache.spark.network.shuffle.RetryingBlockFetcher - Failed to fetch block taskresult_19, and will not retry (0 retries)
java.lang.OutOfMemoryError: Java heap space
	at java.nio.DirectByteBuffer.slice(DirectByteBuffer.java:213)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.nioBuffer(PooledUnsafeDirectByteBuf.java:348)
	at io.netty.buffer.SlicedByteBuf.nioBuffer(SlicedByteBuf.java:271)
	at io.netty.buffer.CompositeByteBuf.nioBuffers(CompositeByteBuf.java:1189)
	at io.netty.buffer.CompositeByteBuf.nioBuffer(CompositeByteBuf.java:1161)
	at io.netty.buffer.AbstractDerivedByteBuf.nioBuffer(AbstractDerivedByteBuf.java:73)
	at io.netty.buffer.AbstractByteBuf.nioBuffer(AbstractByteBuf.java:939)
	at org.apache.spark.network.buffer.NettyManagedBuffer.nioByteBuffer(NettyManagedBuffer.java:45)
	at org.apache.spark.network.BlockTransferService$$anon$1.onBlockFetchSuccess(BlockTransferService.scala:96)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$RetryingBlockFetchListener.onBlockFetchSuccess(RetryingBlockFetcher.java:206)
	at org.apache.spark.network.shuffle.OneForOneBlockFetcher$ChunkCallback.onSuccess(OneForOneBlockFetcher.java:72)
	at org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:150)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:106)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:51)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:86)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
